Both Q-learning and SARSA display negative average rewards at the outset, which is typical as agents explore their environment. SARSA generally yields slightly lower average rewards than Q-learning, possibly due to its on-policy nature, which tends to be more cautious in updating Q-values. Moreover, the reward trends per episode diverge between the two algorithms, with Q-learning exhibiting greater stability and less reward variance compared to SARSA.